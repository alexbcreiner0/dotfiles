\documentclass{beamer}

\usepackage{fancyvrb}
\makeatletter
\def\KV@FV@lastline@default{%
  \let\FancyVerbStopNum\m@ne
  \let\FancyVerbStopString\relax}
\fvset{lastline}
\makeatother
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usetheme{Madrid}

\title{Complexity Theory and NP-Completeness}
\author{Alex Creiner}

\institute{Boston College}
\date{Spring 2024}

\begin{document}

\titlepage
\begin{frame}{Introduction}
    \begin{itemize}
        \item We now turn towards a brief overview of complexity theory more broadly, with the nominal purpose of noting a particular set of especially difficult problems, and how to tell if the problem you are trying to solve is one of them. 
        \item If this material interests you, consider taking my complexity theory elective next spring! 
    \end{itemize}
\end{frame}

\begin{frame}{Decision Problems}
    \begin{itemize}
        \item For the sake of simplicity, computational complexity theorists generally focus on a specific kind of problem known as a \textbf{decision problem}. A decision problem is a problem in which the answer is always a Boolean value: Yes or no, true or false, etc. \pause 
        \item Examples:
        \begin{itemize}
            \item The factoring problem: Given an integer $x$, does it have a non-trivial prime factor? (Non-trivial means not $1$) \pause 
            \item The satisfiability problem (SAT): Given a Boolean expression with $n$ many variables $\phi$, does there exist an assignment of truth values to those variables which makes the expression evaluate to true? (For example, $(x_1 \wedge x_2)\vee \neg x_3$ is satisfiable by setting $x_1$ and $x_2$ equal to True, and $x_3$ equal to false). \pause
            \item The graph isomorphism problem: Given two graphs, is one actually just a relabelling of the nodes of the other? \pause 
        \end{itemize}
        \item The middle of these, $SAT$, is particularly important to us. Don't forget about it. 
    \end{itemize}
\end{frame}

\begin{frame}{Decision Problems}
    \begin{itemize}
        \item Most non-decision problems have a decision problem which can be expressed in place of it, which encapsulates the complexity of the original. \pause 
        \item For example, the travelling salesman problem (TSP) is an optimization problem: Given a complete weighted graph, what is the shortest path which touches every node exactly once? \pause
        \item The decision problem `version' of this is the problem: Given a complete weighted graph and a length $d$, does there exist a path which reaches every node exactly once and which has total length $\leq d$? \pause 
        \item Through the magic of binary search, a solution to the decision problem version of TSP can be utilized to produce a solution to the original optimization question without much time overhead. \pause 
        \item In this way, we should see decision problems as standing in for all other kinds of problems in order to simplify the discussion. 
    \end{itemize}
\end{frame}

\begin{frame}{Some basic complexity classes: \textbf{R}}
    \begin{itemize}
        \item We can now define some basic complexity classes. 
        \item First, we've already briefly mentioned the set of all computable decision problems, $\mathbf{R}$. This is actually a tiny subset of the universe of all decision problems; most problems are \emph{not} computable at all! \pause 
        \item A famous example is the halting problem: Given the code for a computer program and an input to it, will the program ever halt on that input? (I.e. will there not be an infinite loop of some sort?) 
        \item When I teach logic and computation, I always make it a goal to prove that this problem is not computable. 
    \end{itemize}
\end{frame}

\begin{frame}{Some basic complexity classes \textbf{P} and \textbf{EXP}}
    \begin{itemize}
        \item Next, the class $\mathbf{P}$ is the set of all problems which are computable in \textbf{polynomial time}. I.e. a decision problem $L$ is in $\mathbf{P}$ iff there is an algorithm which solves it in time $O(n^k)$ for some $k \in \mathbb{Z}$. 
        \item We've spent the entire class so far safe and cozy within this class. Most of our algorithms usually end up operating in time $O(n^2)$, or $O(n\log(n))$, or $O(n^3)$, etceteral. These are all in $\mathbf{P}$. 
        \item On the other hand, $\mathbf{EXP}$ is the set of problems which are computable in $\textbf{exponential time}$, i.e. time $O(2^{n^k})$ for some $k \in \mathbb{Z}$. 
        \item Since any polynomial time is also exponential time, $\mathbf{P} \subseteq \mathbf{EXP}$. It follows that anything proven to be in $\mathbf{EXP}$ but not in $\mathbf{P}$ could be labelled as an exceptionally difficult problem. Are there any known examples?
    \end{itemize}
\end{frame}

\begin{frame}{Complexity classes continued}
    \begin{itemize}
        \item Plenty! My personal go-to example is generalized chess. Given an $n \times n$ chess board and some configuration of the pieces on that board, does player $1$ have a winning strategy? I.e. is there a move that player $1$ can make so that no matter what move player $2$ makes, there is a move that player $1$ can make so that no matter what move player $2$ makes... etc, until player $1$ wins?
        \item This problem can be proven to be in $\mathbf{EXP}$. In fact, it is $\mathbf{EXP}$-\textbf{complete}. Let's define what that means. \pause 
        \item A problem $P$ is \textbf{hard} for a complexity class if any problem $Q$ in that class can be reduced to $P$ in polynomial time. (i.e. the pre and post-processing required is efficiently computable). 
    \end{itemize}
\end{frame}

\begin{frame}{Completeness}
    \begin{itemize}
        \item If the problem $P$ is itself a member of that complexity class which it is hard for, then we say it is \textbf{complete} for that class. Thus, an $\mathbf{EXP}$-complete problem is representative of the hardest problems in $\mathbf{EXP}$. They represent the maximum difficulty of any problem in the class. Many other games can be proven to be $\mathbf{EXP}$-complete, such as GO. 
        \item I snuck in there that we are assuming all problems in $\mathbf{P}$ to be `efficiently solvable'. That is a pretty major departure from the attitude we've had throughout this class, which is that $O(n^2)$ is `slow'. However, zooming out to this much more encompassing definition of reasonable time complexity gives us some perspective we wouldn't otherwise be able to gain. 
    \end{itemize}
\end{frame}

\begin{frame}{PSPACE}
    \begin{itemize}
        {\small \item The class $\mathbf{PSPACE}$ is the set of all problems which are computable in polynomial \emph{space}. Simple enough. \pause 
        \item One thing which can be immediately noted is that $\mathbf{P} \subseteq \mathbf{PSPACE}$. Why? \pause 
        \item Answer: In time $O(f(n))$ how could you possibly use up space greater than $O(f(n))$ you have to record information down in memory to use space, and you only have $f(n)$ time to do that!
        \item Whether or not $\mathbf{PSPACE} \subseteq \mathbf{P}$ is an open question. However, there is plenty of evidence towards the answer being no. (Still, never say never.) 
        \item However, $\mathbf{PSPACE} \subseteq \mathbf{EXP}$, and provably different. Chess, for example, is in $\mathbf{EXP}$ but not in $\mathbf{P}$. If you want to see why, take my class next spring. 
        \item In fact, $\mathbf{PSPACE}$ is typically regarded as marking the boundary point of anything which could be considered efficiently computable in any way. Outside of $\mathbf{PSPACE}$, nothing is easy. }
    \end{itemize}
\end{frame}

\begin{frame}{NP}
    \begin{itemize}
        \item Finally, let's turn to $\mathbf{NP}$. This is an abbreviation for \textbf{non-deterministic polynomial time}. There are a lot of different ways to conceptualize this class. 
        \item From a practical standpoint, the discussion of $\mathbf{NP}$ is the discussion of the role of \emph{searching} in computer science. We've seen repeatedly in this class a functional equivalence between \emph{searching} and \emph{guessing}. If we have a program that can quickly search for an answer and return it, then from the perspective of $\mathbf{P}$, we might as well have had the program just magically guess the answer in one step. Fast searching is functionally equivalent to guessing. 
        \item What kinds of problems would be efficiently solvable if we had an incredibly lucky machine which could always magically search through a very large set of things for us? Or, equivalently, if we had an incredibly lucky machine that could always guess a thing and have it be what we wanted?
    \end{itemize}
\end{frame}

\begin{frame}{NP continued}
    \begin{itemize}
        \item Sudoku. It would be very hard to program a computer to solve a sudoku puzzle for us. However, what if we guessed at a solution? If we guess, and guess correctly, then it would be \emph{easy} (and \emph{fast}) to \emph{confirm} that our guess was correct. \pause 
        \item This is one way to formulate the class $\mathbf{NP}$: The set of problems such that you can \emph{check to see if you have the right answer} in polynomial time. 
        \item Another example is the Boolean satisfiability problem we mentioned earlier. Given a Boolean expression on $n$ variables, there are $2^n$ possible truth assignments. It would take a long, long time to search through all of these, but \emph{if we could}, or \emph{if we guessed correctly on the first try}, then we could easily confirm in linear time that the answer is yes. 
    \end{itemize}
\end{frame}

\begin{frame}{NP continued}
    \begin{itemize}
        \item So that is $\mathbf{NP}$. $\mathbf{NP}$ is:
        \begin{itemize}
            \item The set of problems such that the solutions can be confirmed in polynomial time.
            \item The set of problems which are efficiently computable \emph{up to} a search subroutine over a very large set.
        \end{itemize}
        \item ($\mathbf{NP}$ is also the set of problems which are computable in polynomial time by a nondeterministic Turing machine; take my class in the spring if you want to know what this means.) 
        \item Clearly, $\mathbf{P} \subseteq \mathbf{NP}$. If you had a polynomial time algorithm for producing a solution, and someone handed you a solution, then you could simply run that polynomial time algorithm and see if you get what they handed you!
    \end{itemize}
\end{frame}

\begin{frame}{The question}
    \begin{itemize}
        \item It's also pretty clear that $\mathbf{NP} \nsubseteq \mathbf{P}$. Why should there be an efficient algorithm for a problem just because there is an efficient way to confirm solutions? Except...
        \item Nobody has ever been able to actually prove this. The question of whether or not $\mathbf{P} = \mathbf{NP}$ remains a frustratingly open question to this day, despite seeming so obviously false. \pause 
        \item This open problem is one of six remaining \href{https://en.wikipedia.org/wiki/Millennium_Prize_Problems}{Millenium Prize Problems}, and has a one million dollar bounty. Of those problems, $\mathbf{P}$ vs $\mathbf{NP}$ is by far the easiest to understand. \pause 
        \item All it would take to prove $\mathbf{P} \neq \mathbf{NP}$ is a single problem in $\mathbf{NP}$ (which is usually very easy to show, as we will see shortly) that is not in $\mathbf{P}$. It's easy enough to find an algorithm for something, but \emph{proving} that no better algorithm exists is much, much harder! 
    \end{itemize}
\end{frame}

\begin{frame}{co-NP}
    \begin{itemize}
        \item Notice the bias here in one direction. We are only saying that solutions can be \emph{confirmed} in polynomial time. We are \emph{not} saying that you can rule out non-solutions in polynomial time. That would be the class $\mathbf{co-NP}$, and these classes are almost certainly different. 
        \item For example, consider the factoring problem from earlier: given an integer $x$ which is $n$ many digits, does it have a nontrivial prime factor?
        \item This problem is clearly in $\mathbf{NP}$. If someone gave you a prime factor, then since division is polynomial time, you could simply perform the division and see if the result is an integer. \pause 
        \item But can we efficiently confirm that $x$ \emph{doesn't} have a non-trivial factor? What could a magic guessing machine give you that would allow you to check this? \pause 
        \item It doesn't seem like there is a way, and so it widely assumed that $\mathbf{NP} \neq \mathbf{co-NP}$ just as it is assumed that $\mathbf{NP} \neq \mathbf{P}$. 
    \end{itemize}
\end{frame}

\begin{frame}{Problems in $\mathbf{NP}$}
    \begin{itemize}
        \item We will (and you should) operate under the assumption that $\mathbf{NP}$ is not equal to $\mathbf{P}$, and therefore that $\mathbf{NP}$-complete problems are not efficiently computable. This is the relevance of $\mathbf{NP}$ to our class. \pause 
        \item By reducing a problem we are trying to solve to an $\mathbf{NP}$-complete problem, we can assume that the problem simply isn't feasible for a computer to solve, and alter our approach. \pause  
        \item Let's therefore look through some $\mathbf{NP}$ problems and try to get a sense of the flavor of this class.
    \end{itemize}
\end{frame}

\begin{frame}{Problems in $\mathbf{NP}$}
    \begin{itemize}
        \item The star of the show for $\mathbf{NP}$, the most important problem in it, is the Boolean satisfiability problem, $SAT$ (which itself is reducible to $3SAT$, which assumes a particular form of the Boolean expression). $SAT$ is $\mathbf{NP}$ complete. 
        \item The sudoku problem we mentioned (generalized to sudoku puzzles of arbitrary sizes) is also $\mathbf{NP}$-complete. 
        \item The factoring problem we mentioned is in $\mathbf{NP}$, but is widely assumed to \emph{not} be $\mathbf{NP}$-complete. It is the prime candidate for an $\mathbf{NP}$-intermediary problem. 
        \item The (decision problem analog of the) travelling salesman problem is $\mathbf{NP}$-complete. 
    \end{itemize}
\end{frame}

\begin{frame}{Knapsack - An interesting case}
    \begin{itemize}
        \item Consider the following problem. Suppose you are robbing a building which has $n$ many items in it. Each item has a dollar value to it, and you want to maximize the value of what you walk away with. But each item also has a weight associated with it, and you can only hold so much, up some maximum weight $W$. This is the knapsack problem: What should you actually put in your backpack and walk away with? \pause 
        \item This problem has a dynamic programming solution. Enumerate the items $i_1,i_2,\ldots,i_n$, along with their weights $w_1,\ldots,w_n$ and values $v_1,\ldots,v_n$. Consider the first item. If it doesn't fit in your backpack, dismiss it. If it does, you have a choice - do we pack it or do we not. What should we do? Guess!
    \end{itemize}
\end{frame}

\begin{frame}{Knapsack - An interesting case}
    \begin{itemize}
        \item Denote the original problem by $K(n,W)$. If we take the item, then we are left with the subproblem $K(n-1,W-w)$. If we don't, then we are left with $K(n-1,W)$. Thus we have the formula:
        \[ K(n,W) = \max\left(K(n-1,W-w), K(n-1,W)\right) \]
        \item How many subproblems are there? \pause $nW$. \pause 
        \item Runtime per subproblem discounting recursion? \pause $O(1)$. 
        \item Therefore the total runtime of this DP is $O(nW)$. Doesn't seem too bad! 
        \item The issue is that $W$ is not the length of the integer input $W$ - it is the actual \emph{number} $W$! If we are measuring runtime strictly in terms of the length of the input as a string, and given an input that is $W$ many \emph{digits}, then the runtime is actually $O(n2^W)$! 
    \end{itemize}
\end{frame}

\begin{frame}{Knapsack - An interesting case}
    \begin{itemize}
        \item In strict terms of input lengths then, this is an exponential time algorithm. Furthermore, the knapsack problem (or rather it's decision problem variant) is provably $\mathbf{NP}$-complete. \pause 
        \item However, we can see that as long as $W$ isn't ridiculously large, this algorithm isn't actually all that bad. It's basically quadratic in $W$ if we regard it by the number and not the length. Problems like this, that are polynomial in the number but not the length, are sometimes called \textbf{Pseudo-polynomial time}. They are technically hard, but for most applications they won't actually appear that way.  
    \end{itemize}
\end{frame}

\begin{frame}{Why SAT?}
    \begin{itemize}
        \item Why is SAT so important over the other $\mathbf{NP}$-complete problems?
        \item Answer: Because relating other problems to SAT tends to be easier than relating them to each other. After all, SAT is just logic, and logic is what everything in math boils down to!
        \item A bit more scaffolding is required to really make good use of SAT however. SAT as we've expressed it is a bit too unstructured/wild. 
        \item Recall that a Boolean expression $\phi$ is in \textbf{conjunctive normal form} (or CNF) if it is a bunch of a big AND of a bunch of ORs. To express this a bit less ridiculously, a \emph{clause} is a set of variables (or their negations) which are OR'd together. So $(x_1 \vee \neg x_2)$ is a clause, as is $(x_1 \vee x_2 \vee x_5)$, etc.
        \item A Boolean expression is in CNF if it is a bunch of these being AND'd together. \pause  
    \end{itemize}
\end{frame}

\begin{frame}{Normalizing SAT}
    \begin{itemize}
        \item The problem of converting an arbitrary Boolean expression into an equivalent Boolean expression in CNF actually takes exponential time in the worst case. However, there \emph{is} a polynomial time algorithm which converts an arbitrary Boolean expression into an expression in CNF which is not equivalent, but such that one is satisfiable if and only if the other is satisfiable (the same expression might not satisfy both). \pause
        \item Thus, the SAT problem is reducible in polynomial time to a special case of the SAT problem in which expressions are assumed to be in conjunctive normal form. This is generally taken as part of the definition of SAT. \pause 
        \item Additionally, we can reduce SAT even further to 3SAT. This is the same problem as SAT except that all clauses are assumed to contain exactly 3 variables (or negations of variables). \pause 
        \item 3SAT is really the big important problem that we are interested in. The extra assumed structure is usually helpful to have.
    \end{itemize}
\end{frame}

\begin{frame}{Proving Hardness and Coping}
    \begin{itemize}
        \item Our game is almost fully set up. Let's summarize. \pause
        \item A problem is $\mathbf{NP}$-complete if it is both in $\mathbf{NP}$ and also $\mathbf{NP}$-hard; i.e. all other problems in $\mathbf{NP}$ are reducible to it. 
        \item $3SAT$ is our `starting point' $\mathbf{NP}$-complete problem. The proof of this is known as the \textbf{Cook-Levin Theorem}, and is sometimes regarded as the \emph{fundamental theorem} of complexity theory.
        \item Suppose we find a way to 3SAT \emph{to} our problem. Then we've shown that our problem is \emph{at least as hard} as 3SAT, in that a solution to our problem could be used to solve 3SAT with no significant time loss.
        \item At that point, we know that our problem is $\mathbf{NP}$-hard, since 3SAT itself is $\mathbf{NP}$-complete. What does this mean?
    \end{itemize}
\end{frame}

\begin{frame}{Proving Hardness and Coping}
    \begin{itemize}
        \item At this point there are one of two possibilities:
        \begin{itemize}
            \item[(1)] Your problem is in $\mathbf{NP}$.
            \item[(2)] Your problem is not in $\mathbf{NP}$.
        \end{itemize}
        \item In case 1, there is technically some hope that your problem is efficiently solvable. But you should still give up, unless are intending to be the one who finally proves $\mathbf{P} = \mathbf{NP}$. Realistically, your problem has no efficient algorithm which solves it in general. However, there are lots of ways of \emph{coping} with $\mathbf{NP}$-completeness that have been developed over the years.
        \item On the other hand, usually the outlook is grim.
        \item However, in either case, you are going to have to change your expectations a bit in order to solve your problem. Let's look at an example of what I mean.
    \end{itemize}
\end{frame}

\begin{frame}{Coping with $\mathbf{NP}$-completeness}
    \begin{itemize}
        \item Let's say your problem is $\mathbf{NP}$-complete, and in fact reduces to the travelling salesman problem. You aren't going to be able to find a minimal path which hits every location exactly once. But if you lower your standards a bit, and/or are able to recognize something particular to your problem that makes it a special case, then you can often still find something. 
        \item For example, are you willing to visit some of the vertices twice?
        \item Additionally, does your particular graph satisfy the \textbf{triangle inequality}? I.e. is the shortest path from one node to another always the most direct one? 
        \item If so, then you can `solve' the problem by using Kruskal's Algorithm to find a minimum spanning tree in time $O(|E|\log(|E|))$ time. This is exactly what it sounds like, a subgraph which is a tree and which reaches every node. 
    \end{itemize}
\end{frame}

\begin{frame}{Coping with $\mathbf{NP}$-completeness}
    \begin{itemize}
        \item If you use Kruskal's Algorithm (or Prim's) to find a MST, then you can visit the nodes in order of a depth first search, and then make your way back up the tree to the original node. 
        \item This will visit some nodes twice, but none more than twice. Furthermore, we can easily show that the total distance travelled this way will be at most twice the actual TSP-cost. Why? The shortest circuit which touches every node once minus the final edge is a path through every vertex which is also a spanning tree. Thus as long as the triangle inequality holds, 
        \begin{center}
            TSP-cost $\geq$ TSP-cost minus final edge weight $\geq$ MST-cost
        \end{center}
        \item In the worst case, our MST is a straight vertical line and all edges are visited twice, making the cost of the circuit twice the cost of the MST, and by the above inequality that is less than twice the length of the shortest travelling salesman route. 
    \end{itemize}
\end{frame}

\begin{frame}{Longest paths}
    \begin{itemize}
        \item This one is hilarious and I am ashamed I didn't know it until now: Longest paths! (Sometimes known as the taxi-cab ripoff problem) \pause 
        \item This is exactly what it sounds like. Given a weighted graph $G$ with nonnegative edge weights, a starting vertex $s$, and a goal vertex $g$, return the length of the longest \emph{simple} path from a starting node to any other node (simple means we aren't allowed to cheat and visit the same node repeatedly). \pause
        \item Surprisingly, multiplying all edges by -1 and using Dijkstra does not work. (I haven't tried to confirm this or think about it yet.) \pause 
        \item Even more surprisingly, despite all of our efficient algorithms for shortest paths, this problem is $\mathbf{NP}$-complete! \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Coping and compromising}
    \begin{itemize}
        \item Once again, compromises and special cases are how we cope with this intractability. \pause 
        \item The special case is, predictably, being a dag. I'm not sure why multiplying edges by -1 doesn't work for Dijkstra, but it does work for our linear time dynamic programming solution for shortest paths when our graph is a dag. \pause 
        \item `But my graph isn't a dag!' I hear you cry. Have you considered sucking it up and \emph{making it} into a dag? We have cheap cycle detection algorithms. Maybe it's time to decide one of the edges making a cycle is an acceptable loss, and getting rid of it. \pause 
        \item One edge lost will probably still result in a good approximation of the longest path from $s$ to $g$, and thus this is a way to cope with $\mathbf{NP}$-completeness.
    \end{itemize}
\end{frame}

\end{document}