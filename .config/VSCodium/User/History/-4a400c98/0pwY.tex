\documentclass{beamer}

\usepackage{fancyvrb}
\makeatletter
\def\KV@FV@lastline@default{%
  \let\FancyVerbStopNum\m@ne
  \let\FancyVerbStopString\relax}
\fvset{lastline}
\makeatother
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{bm}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usetheme{Madrid}

\title{Dynamic Programming}
\subtitle{Recursion round 2}
\author{Alex Creiner}

\institute{Boston College}
\date{Spring 2024}

\begin{document}

\titlepage
\begin{frame}[fragile]{Fibonacci Function Revisited}
    \begin{itemize}
        \item Recall where we left off in the beginning of class when we discussed the Fibonacci sequence. The sequence was defined recursively:
        \begin{align*}
            F_{n}= \begin{cases} 0 & \textrm{if } n = 0 \\ 1 & \textrm{if } n = 1 \\ F_{n-1} + F_{n-2} & else \end{cases} 
        \end{align*}
        \item As always, a recursive mathematical definition always immediately implies a recursive algorithm:
    \end{itemize}
    \begin{minted}{python3}
        def fib(n):
	    if n == 0: return 0
            if n == 1: return 1
            return fib(n-1) + fib(n-2)
    \end{minted}
\end{frame}

\begin{frame}{Fibonacci Function Revisited}
    \begin{itemize}
        \item However, we saw that this algorithm gets out of control rather quickly, and that this solution to the problem as is was exponential runtime. But does it have to be? (Continue on board) \pause 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Memoized Fibonacci}
    \begin{minted}{python3}
        def memoized_fib(n):
            def memoized_fib_aux(n, memo):
                if memo[n-1] == -1:
                    memo[n-1] = memoized_fib_aux(n-1, memo)
                if memo[n-2] == -1:
                    memo[n-2] = memoized_fib_aux(n-2, memo)
                return memo[n-1] + memo[n-2]

            if n == 0: return 0
            if n == 1: return 1
            memo = [-1]*n 
            return memoized_fib_aux(n, memo)
    \end{minted}
\end{frame}

\begin{frame}[fragile]{Pythonized memoization}
    \begin{itemize}
        \item If auxiliary functions aren't your thing, you can instead make use of python's default arguments instead:
    \end{itemize}
    \begin{minted}{python3}
    def pythonized_memo_fib(n, memo = None):
        if n == 0: return 0
        if n == 1: return 1
        if memo == None:
            memo = [-1]*n
            memo[0], memo[1] = 0,1

        if memo[n-1] == -1:
            memo[n-1] = pythonized_memo_fib(n-1, memo)
        if memo[n-2] == -1:
            memo[n-2] = pythonized_memo_fib(n-1, memo)

        return memo[n-1] + memo[n-2]
    \end{minted}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item Since what we're doing here literally is depth first search through a dependency dag, we know that the runtime of the algorithm is $O(|V| + |E|)$. What are $|V|$ and $|E|$ though? \pause 
        \begin{itemize}
            \item $V$, our vertices.\pause We have one for each Fibonacci number before and including the one we are calculating, so $|V| = n+1$. \pause 
            \item $E$, the edges.\pause There are exactly two outgoing edges for every node in our dag \emph{except} for the two base cases, which have none. So $|E| = 2(n-2)$. \pause 
            \item We thus have that $O(|V| + |E|) = O(n+1+2(n-2)) = O(n)$. Linear time! \pause 
        \end{itemize}
        \item This is an extremely dramatic reduction in runtime, but it shouldn't surprise us. After all, we can calculate Fibonacci numbers ourselves in linear time! How is this done? \pause 
        \item Answer: We build our way \emph{up} from the base cases to the number we want. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Top down vs bottom up}
    \begin{itemize}
        \item More generally, we \emph{topologically sort} the dependency dag, and then proceed to calculate the numbers in that sorted order! \pause 
        \item Think about it. We write the numbers \emph{in order}, either in our heads or on paper: $0,1,1,2,3,5,8,\ldots$. What does this even mean, \emph{in order}?
        \item It means we start with $fib(0)$ and $fib(1)$, which we said are sources. Then we write $fib(2)$, which we can because it only needs $fib(2)$ and $fib(1)$, which we have already. Then we write $fib(3)$, which only needs $fib(2)$ and $fib(1)$, which we have already. And so on. \pause 
        \item In this case, the topological sorting is obvious. In general it's not always so easy, as we'll see. \pause 
        \item The memoized solution is also referred to as a \emph{top down} solution.  
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Bottom up solution}
    \begin{minted}{python3}
        def bottom_up_fib(n):
	    if n == 0: return 0
	    if n == 1: return 1
	    memo = [-1]*(n+1)
	    memo[0], memo[1] = 0,1
	    for i in range(2,n+1):
		memo[i] = memo[i-1]+memo[i-2]
    	return memo[n]
    \end{minted}
    \begin{itemize}
        \item It doesn't take any knowledge of graph theory or depth first search to see that this solution is linear time. In general, the bottom up solutions to these problems are much easier to analyze. 
        \item However, these two algorithms are doing the same thing from two different directions! The big-O runtimes of a top-down dynamic programming solution are always the same as the bottom up. \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item The top-down solution is typically slightly slower than the bottom-up one, because of the overhead incurred from repeatedly recursing before finding the base cases as opposed to starting from them (this overhead is effectively performing a topological sort for us!). However, I am doubtful this is always the case. For the most part, whether one implements top-down or bottom-up DP's is generally up to personal preference. \pause  
        \item Since solving a DP problem involves writing down a recursive relationship between a problem and its subproblems, the top-down approach can more quickly translate to actual code, and is thus my personal preference. 
        \item Usually, the topological sorting is obvious (consider the Fibonacci case, for instance), but in any case, going with top-down means you don't need to consider it at all. Aside from being slightly slower, the top-down approach also has the disadvantage of being more error-prone (recursive algorithms tend to be very fragile).
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item The way we drew the dependency dag for the Fibonacci numbers actually has all of the arrows reversed from the one we drew for putting on clothing. (Draw) \pause 
        \item Whereas the top-down solution is performing a depth-first search of this `reversed' dependency dag and constructing the answer in postvisitation order...
        \item The bottom-up solution performing a topological sorting of the dependency dag and then using that linearization to construct the solution afterwards. \pause
        \item Again, if you actually had to perform the toposort before doing the bottom up solution, it would be slower. It's only faster when you can understand the linearization in advance and program your algorithm according to that known ordering. \pause 
        \item This reveals a truth about the relationship between depth-first search and topological sorting: One is basically the other, in reverse! This shouldn't be surprising to us, given the algorithm we found for it.
    \end{itemize}
\end{frame}

\begin{frame}{Shortest paths in dags, revisited}
    \begin{itemize}
        \item Recall that we could find shortest paths in dags in linear time through a peculiar pruning of the relaxations we were doing in Belman-Ford. Let's see if we can make more sense of this now. (Continue on board)
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming Summary}
    \begin{itemize}
        \item With dynamic programming, we are identifying a way to break a problem down in terms of a collection of subproblems. 
        \item Often times, the answer is specifically the biggest or smallest result from a collection of \emph{possible} subproblem breakdowns, as was the case for shortest paths in dags. E.g. we knew that the shortest path to $v$ has to be one of the shortest paths out of all nodes $u$ with edges to $v$ plus the weight of $(u,v)$, but not which one. 
        \item In this case, we can simply \emph{search} over all \emph{possible} shortest paths, and then take a minimum (e.g. brute force). As long as we memoize, we won't waste any time, and for this reason DP should be thought of as `careful brute force'.  
    \end{itemize}
\end{frame}

\begin{frame}{Optimal Substructure}
    \begin{itemize}
        \item Hence the formula you sometimes see: \textbf{dynamic programming = recursion + memoization + searching/guessing}. 
        \item Dynamic programming (in particular the searching part of this equation) is particularly well suited to solving \textbf{optimization problems}, i.e. finding maximums or minimums of some quantity. However, it is not limited to optimization problems. 
        \item The optimization problems which have a dynamic programming solution will demonstrate a property known as \textbf{optimal substructure}: that a solution to a problem `contains within it' optimal solutions to it's subproblems. \pause 
        \item In the case of shortest paths in dags, this amounted to the observation that subpaths of shortest paths are themselves shortest paths. Without this, we wouldn't have the recursion part of the equation. \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming vs Greedy Algorithms vs Divide and Conquer}
    \begin{itemize}
        {\small \item This optimal substructure property also applies to problems which are solved by greedy algorithms. The difference is that greedy algorithms, instead of meticulously searching and recursing, simply make the best looking choice at the moment, and hope that it gets them to the right place. Greedy algorithms for this reason tend to be iterative instead of recursive. 
        \item Most people consider the defining feature of DP to be the the capitalizing on overlapping subproblem structure. If we agree, then we must see divide and conquer as a separate and disjoint programming strategy from DP, since divide and conquer solutions never have overlapping subproblems. In other words, recursion, DP and divide and conquer should all be seen as separate concepts. Both DP and divide and conquer make use of recursion, but are distinct use cases, and recursion itself is broader than both combined.  
        \item Despite the deep theoretical backdrop here, dynamic programming tends to be very mechanical as an approach, but only once you get used to it. Let's try to express the process practically:} 
    \end{itemize}
\end{frame}

\begin{frame}{Steps in Solving a DP}
    \begin{itemize}
        \item[(1)] \textbf{Identify Subproblems}: How is the answer your algorithm supposed to give related to the answers it could give to it's subproblems? Try to express this as a recurrence relation. Before this, you might need to identify a... \pause 
        \item[(2)] \textbf{Guess}: Maybe instead of a having a direct relationship between a problem and it's subproblems, you can only say that the answer is \emph{one of} some set of answers to different collections of subproblems. If the answer is the biggest or smallest of some set of possibilities, then you can wrap a for-loop around your recursion, guess all possibilities, and then pick the one that turned out to be the biggest or smallest.
        \item[(3)] (\textbf{Sort} (Optional)): If you are going for a bottom-up DP, then you need to identify an ordering to solving the subproblems, inductively building yourself up from the base cases to the answer you're looking for. 
    \end{itemize}
\end{frame}

\begin{frame}{Runtime analysis and avoiding cycles}
    \begin{itemize}
        \item[(4)] \textbf{Runtime analysis}: This isn't really a step, but it's something you should think about! Usually, the runtime of a DP is very easy to find. By linearity of DFS, this is simply
        \[ \textrm{Runtime} = (\textrm{Number of subproblems})\times (\textrm{Time time per subproblem*}) \]
        \item The asterisk is that your time per subproblem should \emph{ignore} the recursive calls, i.e. consider them to be constant time. Otherwise you will be double counting! 
        \item This is a `quick and dirty' way of obtaining runtimes, and only gives tight bounds when the work you have to do per subproblem is mostly the same for each subproblem.
        \item Finally, you should always take a moment to make sure that the subproblem dag you've defined is acyclic. If it isn't, then your DP will not work and will result in an infinite loop!
    \end{itemize}
\end{frame}

\begin{frame}{Example time}
    \begin{itemize}
        \item Continuing on board. 
    \end{itemize}
\end{frame}

\begin{frame}{So what is the deal with recursion???}
    \begin{itemize}
        \item If there's been a `main character' of this class up to now, it's been recursion. It wouldn't be right at this point not to reflect a little bit on the concept philosophically. \pause 
        \item At the heart of recusion is the notion of \textbf{embedded self-similarity}. By this we mean recursion exists whenever we are dealing with systems or structure that is composed of substructures that are themselves smaller versions of the same thing. \pause
        \item This notion is pervasive, not just across the field of problems we want computers to solve, but also across all natural and man-made systems of all kinds. I'd like to mention at least one example of this.
    \end{itemize}
\end{frame}

\begin{frame}{Viable Systems}
    \begin{itemize}
        \item The concept of a viable system is an extremely general one. A system (which we leave intentionally ambiguous) is called \textbf{viable} if it capable of continually producing and maintaining itself against the backdrop of a changing environment. \pause 
        \item We are all viable systems. Our bodies are continually reproducing our cells as they die. Every seven years we are effectively entirely different people. This process is called \textbf{autopoiesis}.
        \item All systems we wish to construct are also viable systems, for obvious reasons. Of course, no system is truly viable on an infinite time scale, but as an ideal it is a valuable modelling concept to work with. Why? Because it is recursive.
        \item Suppose a system is viable, and consider it's vital subsystems. What can be said about them? \pause 
        \item Answer: They must themselves be viable systems! Thus the notion of a viable system is fundamentally recursive, by logical necessity. 
    \end{itemize}
\end{frame}

\begin{frame}{The Viable Systems Model}
    \begin{itemize}
        \item The inventor of the viable systems model is a man named Stafford Beer, a cybernetician who was hired by businesses to help them organize more efficiently. He would help them project their business model onto his viable systems model in order to analyze them and diagnose deficiencies. This model isn't useful because it is correct (no model is), but because it helps to illuminate necessities towards viability which might otherwise be overlooked. \par 
        \item This is a very broadly applicable model, clearly, and others noticed it as well. In 1970 the Chilean government under president Salvadore Allende contracted Beer to help them model their own national economy as part of a plan to transform their system away from the conventional capitalist model we are currently stuck in. \par 
    \end{itemize}
\end{frame}

\begin{frame}{Example: The human body}
    \begin{itemize}
        {\small \item The viable systems model is itself a model generalized from the human body, in which there are 5 fundamental subsystems. System 5 is the brain, making big executive decisions. 
        \item System 4 is the medium between the brain and the brainstem. It aggregates and filters information, and decides what the brain `sees' and `hears'. It also compiles and carries decisions made by system 5 down to the lower subsystems. 
        \item System 3 is essentially the brainstem. It's job is to take the decisions passed down from system 4 and translate them to actual coordinated action by the organs, as well as gather data to pass up to system 4. 
        \item System 2 is the central nervous system. It synchronizes and coordinates the system 1's, of which there are many. 
        \item Finally, each of the individual organs is system 1. The stomach, the hands and feet, and so on. Recursively, these system 1's are all their own viable systems, with an identical structure to what we just described.}
    \end{itemize}
\end{frame}

\begin{frame}{Examples}
    \begin{itemize}
        {\small \item The first night he was hired, Beer and his colleagues modelled things as follows.
        \item The world economy was a viable system (again, we say this not because it's true, but because it is useful for finding the specific reasons why it isn't). The world economy is composed of many national economies, with their individual governments the system 5. 
        \item The system 1's of these governments were departments, e.g. departments of health, education, finance, etc. One of these is the department of industry. 
        \item The system 1's of this department are the subdepartments for the individual industries, such as food, textiles, automotive, etc. 
        \item Zooming in on, say, the automotives industry, the system 1's would be the individual factories producing the vehicles. The system 1's of these factories are departments, composed of social units. 
        \item The system 1's of these social units are individuals, human beings. These people are viable systems as we described above, and their system 1's are organs. 
        \item These organs are viable systems, their system 1's being cells. And so on, and so on. }
    \end{itemize}
\end{frame}

\begin{frame}{Recursion is very powerful, and very general}
    \begin{itemize}
        \item I mention this to make the point that recursive thinking is a very powerful and self-evidently correct way to think about almost anything. 
        \item As we've seen repeatedly throughout the semester, just finding out how to think of something recursively often leads to an immediate solution to whatever problem led you to thinking about it. This is as true of Beer's viable systems model as it is for us. 
        \item The recursive functions are in fact a theoretical model of computation in themselves, and that model is Turing complete! Every problem that is computable can be solved recursively. (This model has a little more going on than just recursion, but not as much as you would think!)
        \item In fact, the set of all computable problems is usually denoted $\mathbf{R}$. You can take a guess what the R stands for! 
    \end{itemize}     
\end{frame}

\end{document}