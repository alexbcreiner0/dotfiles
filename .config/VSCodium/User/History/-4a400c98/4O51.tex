\documentclass{beamer}

\usepackage{fancyvrb}
\makeatletter
\def\KV@FV@lastline@default{%
  \let\FancyVerbStopNum\m@ne
  \let\FancyVerbStopString\relax}
\fvset{lastline}
\makeatother
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{bm}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usetheme{Madrid}

\title{Dynamic Programming}
\subtitle{Recursion round 2}
\author{Alex Creiner}

\institute{Boston College}
\date{Spring 2024}

\begin{document}

\titlepage
\begin{frame}[fragile]{Fibonacci Function Revisited}
    \begin{itemize}
        \item Recall where we left off in the beginning of class when we discussed the Fibonacci sequence. The sequence was defined recursively:
        \begin{align*}
            F_{n}= \begin{cases} 0 & \textrm{if } n = 0 \\ 1 & \textrm{if } n = 1 \\ F_{n-1} + F_{n-2} & else \end{cases} 
        \end{align*}
        \item As always, a recursive mathematical definition always immediately implies a recursive algorithm:
    \end{itemize}
    \begin{minted}{python3}
        def fib(n):
	    if n == 0: return 0
            if n == 1: return 1
            return fib(n-1) + fib(n-2)
    \end{minted}
\end{frame}

\begin{frame}{Fibonacci Function Revisited}
    \begin{itemize}
        \item However, we saw that this algorithm gets out of control rather quickly, and that this solution to the problem as is was exponential runtime. But does it have to be? (Continue on board) \pause 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Memoized Fibonacci}
    \begin{minted}{python3}
        def memoized_fib(n):
            def memoized_fib_aux(n, memo):
                if memo[n-1] == -1:
                    memo[n-1] = memoized_fib_aux(n-1, memo)
                if memo[n-2] == -1:
                    memo[n-2] = memoized_fib_aux(n-2, memo)
                return memo[n-1] + memo[n-2]

            if n == 0: return 0
            if n == 1: return 1
            memo = [-1]*n 
            return memoized_fib_aux(n, memo)
    \end{minted}
\end{frame}

\begin{frame}[fragile]{Pythonized memoization}
    \begin{itemize}
        \item If auxiliary functions aren't your thing, you can instead make use of python's default arguments instead:
    \end{itemize}
    \begin{minted}{python3}
    def pythonized_memo_fib(n, memo = None):
        if n == 0: return 0
        if n == 1: return 1
        if memo == None:
            memo = [-1]*n
            memo[0], memo[1] = 0,1

        if memo[n-1] == -1:
            memo[n-1] = pythonized_memo_fib(n-1, memo)
        if memo[n-2] == -1:
            memo[n-2] = pythonized_memo_fib(n-1, memo)

        return memo[n-1] + memo[n-2]
    \end{minted}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item Since what we're doing here literally is depth first search through a dependency dag, we know that the runtime of the algorithm is $O(|V| + |E|)$. What are $|V|$ and $|E|$ though? \pause 
        \begin{itemize}
            \item $V$, our vertices.\pause We have one for each Fibonacci number before and including the one we are calculating, so $|V| = n+1$. \pause 
            \item $E$, the edges.\pause There are exactly two outgoing edges for every node in our dag \emph{except} for the two base cases, which have none. So $|E| = 2(n-2)$. \pause 
            \item We thus have that $O(|V| + |E|) = O(n+1+2(n-2)) = O(n)$. Linear time! \pause 
        \end{itemize}
        \item This is an extremely dramatic reduction in runtime, but it shouldn't surprise us. After all, we can calculate Fibonacci numbers ourselves in linear time! How is this done? \pause 
        \item Answer: We build our way \emph{up} from the base cases to the number we want. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Top down vs bottom up}
    \begin{itemize}
        \item More generally, we \emph{topologically sort} the dependency dag, and then proceed to calculate the numbers in that sorted order! \pause 
        \item Think about it. We write the numbers \emph{in order}, either in our heads or on paper: $0,1,1,2,3,5,8,\ldots$. What does this even mean, \emph{in order}?
        \item It means we start with $fib(0)$ and $fib(1)$, which we said are sources. Then we write $fib(2)$, which we can because it only needs $fib(2)$ and $fib(1)$, which we have already. Then we write $fib(3)$, which only needs $fib(2)$ and $fib(1)$, which we have already. And so on. \pause 
        \item In this case, the topological sorting is obvious. In general it's not always so easy, as we'll see. \pause 
        \item The memoized solution is also referred to as a \emph{top down} solution.  
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Bottom up solution}
    \begin{minted}{python3}
        def bottom_up_fib(n):
	    if n == 0: return 0
	    if n == 1: return 1
	    memo = [-1]*(n+1)
	    memo[0], memo[1] = 0,1
	    for i in range(2,n+1):
		memo[i] = memo[i-1]+memo[i-2]
    	return memo[n]
    \end{minted}
    \begin{itemize}
        \item It doesn't take any knowledge of graph theory or depth first search to see that this solution is linear time. In general, the bottom up solutions to these problems are much easier to analyze. 
        \item However, these two algorithms are doing the same thing from two different directions! The big-O runtimes of a top-down dynamic programming solution are always the same as the bottom up. \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item The top-down solution is typically slightly slower than the bottom-up one, because of the overhead incurred from repeatedly recursing before finding the base cases as opposed to starting from them (this overhead is effectively performing a topological sort for us!). However, I am doubtful this is always the case. For the most part, whether one implements top-down or bottom-up DP's is generally up to personal preference. \pause  
        \item Since solving a DP problem involves writing down a recursive relationship between a problem and its subproblems, the top-down approach can more quickly translate to actual code, and is thus my personal preference. 
        \item Usually, the topological sorting is obvious (consider the Fibonacci case, for instance), but in any case, going with top-down means you don't need to consider it at all. Aside from being slightly slower, the top-down approach also has the disadvantage of being more error-prone (recursive algorithms tend to be very fragile).
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item The way we drew the dependency dag for the Fibonacci numbers actually has all of the arrows reversed from the one we drew for putting on clothing. (Draw) \pause 
        \item Whereas the top-down solution is performing a depth-first search of this `reversed' dependency dag and constructing the answer in postvisitation order...
        \item The bottom-up solution performing a topological sorting of the dependency dag and then using that linearization to construct the solution afterwards. \pause
        \item Again, if you actually had to perform the toposort before doing the bottom up solution, it would be slower. It's only faster when you can understand the linearization in advance and program your algorithm according to that known ordering. \pause 
        \item This reveals a truth about the relationship between depth-first search and topological sorting: One is basically the other, in reverse! This shouldn't be surprising to us, given the algorithm we found for it.
    \end{itemize}
\end{frame}

\begin{frame}{Shortest paths in dags, revisited}
    \begin{itemize}
        \item Recall that we could find shortest paths in dags in linear time through a peculiar pruning of the relaxations we were doing in Belman-Ford. Let's see if we can make more sense of this now. (Continue on board)
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming Summary}
    \begin{itemize}
        \item With dynamic programming, we are identifying a way to break a problem down in terms of a collection of subproblems. 
        \item Often times, the answer is specifically the biggest or smallest result from a collection of \emph{possible} subproblem breakdowns, as was the case for shortest paths in dags. E.g. we knew that the shortest path to $v$ has to be one of the shortest paths out of all nodes $u$ with edges to $v$ plus the weight of $(u,v)$, but not which one. 
        \item In this case, we can simply \emph{search} over all \emph{possible} shortest paths, and then take a minimum (e.g. brute force). As long as we memoize, we won't waste any time, and for this reason DP should be thought of as `careful brute force'.  
    \end{itemize}
\end{frame}

\begin{frame}{Optimal Substructure}
    \begin{itemize}
        \item Hence the formula you sometimes see: \textbf{dynamic programming = recursion + memoization + searching/guessing}. 
        \item Dynamic programming (in particular the searching part of this equation) is particularly well suited to solving \textbf{optimization problems}, i.e. finding maximums or minimums of some quantity. However, it is not limited to optimization problems. 
        \item The optimization problems which have a dynamic programming solution will demonstrate a property known as \textbf{optimal substructure}: that a solution to a problem `contains within it' optimal solutions to it's subproblems. \pause 
        \item In the case of shortest paths in dags, this amounted to the observation that subpaths of shortest paths are themselves shortest paths. Without this, we wouldn't have the recursion part of the equation. \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming vs Greedy Algorithms vs Divide and Conquer}
    \begin{itemize}
        {\small \item This optimal substructure property also applies to problems which are solved by greedy algorithms. The difference is that greedy algorithms, instead of meticulously searching and recursing, simply make the best looking choice at the moment, and hope that it gets them to the right place. Greedy algorithms for this reason tend to be iterative instead of recursive. 
        \item Most people consider the defining feature of DP to be the the capitalizing on overlapping subproblem structure. If we agree, then we must see divide and conquer as a separate and disjoint programming strategy from DP, since divide and conquer solutions never have overlapping subproblems. In other words, recursion, DP and divide and conquer should all be seen as separate concepts. Both DP and divide and conquer make use of recursion, but are distinct use cases, and recursion itself is broader than both combined.  
        \item Despite the deep theoretical backdrop here, dynamic programming tends to be very mechanical as an approach, but only once you get used to it. Let's therefore turn to doing more examples.} 
    \end{itemize}
\end{frame}

\begin{frame}{Steps in Solving a DP}
    \begin{itemize}
        \item[(1)] \textbf{Identify Subproblems}: How is the answer your algorithm supposed to give related to the answers it could give to it's subproblems? Try to express this as a recurrence relation. Before this, you might need to identify a... \pause 
        \item[(2)] \textbf{Guess}: Maybe instead of a having a direct relationship between a problem and it's subproblems, you can only say that the answer is \emph{one of} some set of answers to different collections of subproblems. If the answer is the biggest or smallest of some set of possibilities, then you can wrap a for-loop around your recursion, guess all possibilities, and then pick the one that turned out to be the biggest or smallest.
        \item[(3)] (\textbf{Sort} (Optional)): If you are going for a bottom-up DP, then you need to identify an ordering to solving the subproblems, inductively building yourself up from the base cases to the answer you're looking for. 
    \end{itemize}
\end{frame}

\begin{frame}{Runtime analysis and avoiding cycles}
    \begin{itemize}
        \item[(4)] \textbf{Runtime analysis}: This isn't really a step, but it's something you should think about! Usually, the runtime of a DP is very easy to find. By linearity of DFS, this is simply
        \[ \textrm{Runtime} = (\textrm{Number of subproblems})\times (\textrm{Time time per subproblem*}) \]
        \item The asterisk is that your time per subproblem should \emph{ignore} the recursive calls, i.e. consider them to be constant time. Otherwise you will be double counting! 
        \item This is a `quick and dirty' way of obtaining runtimes, and only gives tight bounds when the work you have to do per subproblem is mostly the same for each subproblem.
        \item Finally, you should always take a moment to make sure that the subproblem dag you've defined is acyclic. If it isn't, then your DP will not work and will result in an infinite loop!
    \end{itemize}
\end{frame}

\begin{frame}{(Concluding thoughts: Recursion is a very deep concept}
    \begin{itemize}
        \item In theoretical CS (as a subfield of mathematics), we characterize classes of problems. We also generally stick to a particular kind of problem known as \textbf{decision problems}.
        \item A decision problem is a problem in which the answer (i.e. output) is always a true or false. I.e. the problem is a yes or no question. 
        \item Most problems have analogous decision problems which match the complexity of the original problem. For example, the travelling salesman problem (TSP) is the problem: given a complete weighted graph, what is the shortest path which visits every vertex once? 
        \item The decision problem variant of TSP is: Given a complete weighted graph and a number $d$, \emph{is there} a path which touches each vertex exactly once of length less than or equal to $d$?
        \item Through the use of binary search, a solution to the decision problem version of TSP can be converted efficiently into a solution to the problem itself. 
    \end{itemize}
\end{frame}

\begin{frame}{Concluding thoughts continued}
    \begin{itemize}
        \item Since most problems can be reexpressed as decision problem as above, mathematicians tend to restrict their attention to decision problems specifically. They do this because decision problems can be seen very easily as mathematical objects. 
        \item For example, you can view a decision problem as the set of all of the strings corresponding to yes answers. Alternatively, you can see them as infinite sequences of $0$'s and $1$'s, where the $k^{th}$ digit of the sequence represents whether or not the answer is a yes or a no. 
        \item We can then define the \textbf{set} of all computable decision problems. This is typically denoted $\mathbf{R}$. What does the R stand for, you ask?
        \item Another name for this set is the recursive functions. Recursion in fact defines a whole model of computation, and this model is Turing complete, i.e. as powerful as \emph{any other}! 
        \item Recursion is a very deep concept indeed. 
    \end{itemize}
\end{frame}

\end{document}