\documentclass{beamer}

\usepackage{fancyvrb}
\makeatletter
\def\KV@FV@lastline@default{%
  \let\FancyVerbStopNum\m@ne
  \let\FancyVerbStopString\relax}
\fvset{lastline}
\makeatother
\usepackage{listings}
\usepackage{minted}
\usepackage{tikz}
\usepackage{graphicx}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}
\usetheme{Madrid}

\title{Dynamic Programming}
\subtitle{Finally taming recursion for good}
\author{Alex Creiner}

\institute{Boston College}
\date{Spring 2024}

\begin{document}

\titlepage
\begin{frame}[fragile]{Fibonacci Function Revisited}
    \begin{itemize}
        \item Recall where we left off in the beginning of class when we discussed the Fibonacci sequence. The sequence was defined recursively:
        \begin{align*}
            F_{n}= \begin{cases} 0 & \textrm{if } n = 0 \\ 1 & \textrm{if } n = 1 \\ F_{n-1} + F_{n-2} & else \end{cases} 
        \end{align*}
        \item As always, a recursive mathematical definition always immediately implies a recursive algorithm:
    \end{itemize}
    \begin{minted}{python3}
        def fib(n):
	    if n == 0: return 0
            if n == 1: return 1
            return fib(n-1) + fib(n-2)
    \end{minted}
\end{frame}

\begin{frame}{Fibonacci Function Revisited}
    \begin{itemize}
        \item However, we saw that this algorithm gets out of control rather quickly, and that this solution to the problem as is was exponential runtime. But does it have to be? (Continue on board) \pause 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Memoized Fibonacci}
    \begin{minted}{python3}
        def memoized_fib(n):
            def memoized_fib_aux(n, memo):
                if memo[n-1] == -1:
                    memo[n-1] = memoized_fib_aux(n-1, memo)
                if memo[n-2] == -1:
                    memo[n-2] = memoized_fib_aux(n-2, memo)
                return memo[n-1] + memo[n-2]

            if n == 0: return 0
            if n == 1: return 1
            memo = [-1]*n 
            return memoized_fib_aux(n, memo)
    \end{minted}
\end{frame}

\begin{frame}[fragile]{Pythonized memoization}
    \begin{itemize}
        \item If auxiliary functions aren't your thing, you can instead make use of python's default arguments instead:
    \end{itemize}
    \begin{minted}{python3}
    def pythonized_memo_fib(n, memo = None):
        if n == 0: return 0
        if n == 1: return 1
        if memo == None:
            memo = [-1]*n
            memo[0], memo[1] = 0,1

        if memo[n-1] == -1:
            memo[n-1] = pythonized_memo_fib(n-1, memo)
        if memo[n-2] == -1:
            memo[n-2] = pythonized_memo_fib(n-1, memo)

        return memo[n-1] + memo[n-2]
    \end{minted}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item Since what we're doing here literally is depth first search through a dependency dag, we know that the runtime of the algorithm is $O(|V| + |E|)$. What are $|V|$ and $|E|$ though? \pause 
        \begin{itemize}
            \item $V$, our vertices.\pause We have one for each Fibonacci number before and including the one we are calculating, so $|V| = n+1$. \pause 
            \item $E$, the edges.\pause There are exactly two outgoing edges for every node in our dag \emph{except} for the two base cases, which have none. So $|E| = 2(n-2)$. \pause 
            \item We thus have that $O(|V| + |E|) = O(n+1+2(n-2)) = O(n)$. Linear time! \pause 
        \end{itemize}
        \item This is an extremely dramatic reduction in runtime, but it shouldn't surprise us. After all, we can calculate Fibonacci numbers ourselves in linear time! How is this done? \pause 
        \item Answer: We build our way \emph{up} from the base cases to the number we want. 
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Top down vs bottom up}
    \begin{itemize}
        \item More generally, we \emph{topologically sort} the dependency dag, and then proceed to calculate the numbers in that sorted order! \pause 
        \item Think about it. We write the numbers \emph{in order}, either in our heads or on paper: $0,1,1,2,3,5,8,\ldots$. What does this even mean, \emph{in order}?
        \item It means we start with $fib(0)$ and $fib(1)$, which we said are sources. Then we write $fib(2)$, which we can because it only needs $fib(2)$ and $fib(1)$, which we have already. Then we write $fib(3)$, which only needs $fib(2)$ and $fib(1)$, which we have already. And so on. \pause 
        \item In this case, the topological sorting is obvious. In general it's not always so easy, as we'll see. \pause 
        \item The memoized solution is also referred to as a \emph{top down} solution.  
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Bottom up solution}
    \begin{minted}{python3}
        def bottom_up_fib(n):
	    if n == 0: return 0
	    if n == 1: return 1
	    memo = [-1]*(n+1)
	    memo[0], memo[1] = 0,1
	    for i in range(2,n+1):
		memo[i] = memo[i-1]+memo[i-2]
    	return memo[n]
    \end{minted}
    \begin{itemize}
        \item It doesn't take any knowledge of graph theory or depth first search to see that this solution is linear time. In general, the bottom up solutions to these problems are much easier to analyze. 
        \item However, these two algorithms are doing the same thing from two different directions! The big-O runtimes of a top-down dynamic programming solution are always the same as the bottom up. \pause 
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item That said, the top-down solution is typically slower than the bottom-up one, because of the overhead incurred from repeatedly recursing before finding the base cases as opposed to starting from them. \pause  
        \item The top-down solution is typically what we have first, since all of this is implied by having a recurrence relation which relates the solution to a problem to it's subproblems. That overhead is effectively performing a topological sort for us and constructing the solution from the bottom up at the same time! 
        \item If the topological sorting is obvious, as it is in the Fibonacci case, then it is generally going to be a better idea to go with the iterative bottom-up solution than the top-down recursive one. But if you're lazy and have the runtime to spare, the top-down is usually very easy to program!
    \end{itemize}
\end{frame}

\begin{frame}{Top down vs bottom up}
    \begin{itemize}
        \item The way we drew the dependency dag for the Fibonacci numbers actually has all of the arrows reversed from the one we drew for putting on clothing. (Draw) \pause 
        \item Whereas the top-down solution is performing a depth-first search of this `reversed' dependency dag and constructing the answer in postvisitation order...
        \item The bottom-up solution performing a topological sorting of the dependency dag and then using that linearization to construct the solution afterwards. \pause
        \item Again, if you actually had to perform the toposort before doing the bottom up solution, it would be slower. It's only faster when you can understand the linearization in advance and program your algorithm according to that known ordering. \pause 
        \item This reveals a truth about the relationship between depth-first search and topological sorting: One is basically the other, in reverse! This shouldn't be surprising to us, given the algorithm we found for it.
    \end{itemize}
\end{frame}

\begin{frame}{Shortest paths in dags, revisited}
    \begin{itemize}
        \item Recall that we could find shortest paths in dags in linear time through a peculiar pruning of the relaxations we were doing in Belman-Ford. Let's see if we can make more sense of this now. (Continue on board)
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming Summary}
    \begin{itemize}
        \item I'm a little bit unsold on giving this topic it's own name. What we are talking about is just simply recursion. The distinction between recursion and dynamic programming is effectively the distinction between the recursive function we called explore, and the wrapper function we called depth-first search. 
        \item With dynamic programming, we are identifying a way to break a problem down in terms of a collection of subproblems. 
        \item Often times, the answer is specifically the biggest or smallest result from a collection of \emph{possible} subproblem breakdowns, as was the case for shortest paths in dags. E.g. we knew that the shortest path to $v$ has to be one of the shortest paths out of all nodes $u$ with edges to $v$ plus the weight of $(u,v)$, but not which one. 
        \item In this case, we can simply \emph{search} over all \emph{possible} shortest paths, and then take a minimum. As long as we memoize, we won't waste any time. 
    \end{itemize}
\end{frame}

\begin{frame}{Dynamic Programming Summary}
    \begin{itemize}
        \item Hence the formula you sometimes see: \textbf{dynamic programming = recursion + memoization + searching}. 
        \item Dynamic programming (in particular the searching part of this equation) is particularly well suited to solving \textbf{optimization problems}, i.e. finding maximums or minimums of some quantity. 
        \item The optimization problems which have a dynamic programming solution will demonstrate a property known as \textbf{optimal substructure}: that a solution to a problem `contains within it' optimal solutions to it's subproblems. \pause 
        \item In the case of shortest paths in dags, this amounted to the observation that subpaths of shortest paths are themselves shortest paths. Without this, we wouldn't have the recursion part of the equation. \pause 
        \item This optimal substructure property also applies to problems which are solved by greedy algorithms. The difference is that greedy algorithms, instead of meticulously searching, instead make the best looking choice at the moment, and simply hope that it gets them to the right place. 
    \end{itemize}
\end{frame}

\end{document}